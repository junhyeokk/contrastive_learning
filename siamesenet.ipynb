{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e4e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/main/siamese_network/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044bc223",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms as T\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5147493a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f5108ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        self.resnet = torchvision.models.resnet18(weights = None)\n",
    "        \n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size = (7, 7), stride = (2, 2), padding = (3, 3), bias = False)\n",
    "        self.fc_in_features = self.resnet.fc.in_features\n",
    "        \n",
    "        self.resnet = torch.nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.fc_in_features * 2, 256),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.resnet.apply(self.init_weights)\n",
    "        self.fc.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        output = self.resnet(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        return output\n",
    "    \n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        \n",
    "        output = torch.cat((output1, output2), 1)\n",
    "        \n",
    "        output = self.fc(output)\n",
    "        \n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "786c2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class APP_MATCHER(Dataset):\n",
    "    def __init__(self, root, train, download = False):\n",
    "        super(APP_MATCHER, self).__init__()\n",
    "        \n",
    "        mnist_transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.5,), (1.0,))\n",
    "        ])\n",
    "        \n",
    "        self.dataset = datasets.MNIST(root, transform = mnist_transform, train = train, download = download)\n",
    "        \n",
    "        self.data = self.dataset.data.unsqueeze(1).clone()\n",
    "        \n",
    "        self.group_examples()\n",
    "        \n",
    "    def group_examples(self):\n",
    "        np_arr = np.array(self.dataset.targets.clone())\n",
    "        \n",
    "        self.grouped_examples = {}\n",
    "        for i in range(0, 10):\n",
    "            self.grouped_examples[i] = np.where((np_arr == i))[0]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        selected_class = random.randint(0, 9)\n",
    "        \n",
    "        random_index_1 = random.randint(0, self.grouped_examples[selected_class].shape[0] - 1)\n",
    "        \n",
    "        index_1 = self.grouped_examples[selected_class][random_index_1]\n",
    "        \n",
    "        image_1 = self.data[index_1].clone().float()\n",
    "        \n",
    "        # same class\n",
    "        if index % 2 == 0:\n",
    "            random_index_2 = random.randint(0, self.grouped_examples[selected_class].shape[0] - 1)\n",
    "            \n",
    "            while random_index_2 == random_index_1:\n",
    "                random_index_2 = random.randint(0, self.grouped_examples[selected_class].shape[0] - 1)\n",
    "            \n",
    "            index_2 = self.grouped_examples[selected_class][random_index_2]\n",
    "            \n",
    "            image_2 = self.data[index_2].clone().float()\n",
    "            \n",
    "            target = torch.tensor(1, dtype = torch.float)\n",
    "        \n",
    "        # other class\n",
    "        else:\n",
    "            other_selected_class = random.randint(0, 9)\n",
    "            \n",
    "            while other_selected_class == selected_class:\n",
    "                other_selected_class = random.randint(0, 9)\n",
    "                \n",
    "            random_index_2 = random.randint(0, self.grouped_examples[other_selected_class].shape[0] - 1)\n",
    "            \n",
    "            index_2 = self.grouped_examples[other_selected_class][random_index_2]\n",
    "            \n",
    "            image_2 = self.data[index_2].clone().float()\n",
    "            \n",
    "            target = torch.tensor(0, dtype = torch.float)\n",
    "        \n",
    "        return image_1, image_2, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58cd6d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for batch_idx, (images_1, images_2, targets) in enumerate(train_loader):\n",
    "        images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images_1, images_2).squeeze() # squeeze ?\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(images_1), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4719779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (images_1, images_2, targets) in test_loader:\n",
    "            images_1, images_2, targets = images_1.to(device), images_2.to(device), targets.to(device)\n",
    "            outputs = model(images_1, images_2).squeeze()\n",
    "            test_loss += criterion(outputs, targets).sum().item()\n",
    "            pred = torch.where(outputs > 0.5, 1, 0)\n",
    "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print(\"\\nTest set : Average loss : {:.4f}, Accuracy : {}/{} ({:.0f}%)\\n\".format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72881eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_mps = torch.backends.mps.is_available()\n",
    "\n",
    "print(use_cuda)\n",
    "print(use_mps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284a6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "elif use_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2ffae5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "test_batch_size = 32\n",
    "lr = 0.001\n",
    "gamma = 0.7\n",
    "epochs = 10\n",
    "log_interval = 1000\n",
    "\n",
    "train_kwargs = {\"batch_size\" : batch_size}\n",
    "test_kwargs = {\"batch_size\" : test_batch_size}\n",
    "\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {\n",
    "#         \"num_workers\" : 0,\n",
    "                  \"pin_memory\" : True,\n",
    "                  \"shuffle\" : True\n",
    "    }\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e74ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = APP_MATCHER(\"./data\", train = True, download = True)\n",
    "test_dataset = APP_MATCHER(\"./data\", train = False)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, **test_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6028f671",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr = lr)\n",
    "scheduler = StepLR(optimizer, step_size = 1, gamma = gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e53af3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "image_1, image_2, label = next(iter(train_loader))\n",
    "print(image_2[0].shape)\n",
    "print(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "551d9f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 28, 28])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "mnist_transform = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.5,), (1.0,))\n",
    "])\n",
    "\n",
    "forward_once_dataset = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\"./data\", transform = mnist_transform,train = False, download = True),\n",
    "    batch_size = 16\n",
    ")\n",
    "\n",
    "images, targets = next(iter(forward_once_dataset))\n",
    "print(images.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1943a72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(torch.squeeze(images[0]).numpy(), cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bd0a030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([16, 512])\n",
      "tensor([[0.0307, 0.0000, 0.0667,  ..., 0.3817, 0.0000, 0.0000],\n",
      "        [0.0507, 0.0000, 0.0225,  ..., 0.5306, 0.0146, 0.0000],\n",
      "        [0.0465, 0.0000, 0.0180,  ..., 0.3555, 0.0903, 0.0000],\n",
      "        ...,\n",
      "        [0.0430, 0.0000, 0.0373,  ..., 0.4985, 0.0000, 0.0000],\n",
      "        [0.0550, 0.0000, 0.0031,  ..., 0.5607, 0.0342, 0.0000],\n",
      "        [0.0325, 0.0000, 0.1027,  ..., 0.3633, 0.0438, 0.0000]],\n",
      "       grad_fn=<ToCopyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(images[0].unsqueeze(0).shape)\n",
    "out = model.forward_once(images.to(device))\n",
    "print(out.cpu().shape)\n",
    "print(out.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70b0097e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 2)\n",
      "[[ 0.3683178   0.36992124]\n",
      " [-0.34976804  0.8451909 ]\n",
      " [ 1.7242029   0.4254024 ]\n",
      " [-1.5077126   0.34012395]\n",
      " [ 0.425223   -0.60013217]\n",
      " [ 1.2671986   0.30416873]\n",
      " [-0.21392053 -0.6628214 ]\n",
      " [ 0.21546504 -0.45253596]\n",
      " [-0.46810576 -0.23584354]\n",
      " [-0.6306114  -0.7351389 ]\n",
      " [-1.1385658   0.5759301 ]\n",
      " [-0.65079373 -0.64697444]\n",
      " [ 0.01366094 -0.51530445]\n",
      " [-0.91124725  0.9378941 ]\n",
      " [ 1.4310971   0.26376754]\n",
      " [ 0.42555943 -0.21364826]]\n",
      "[7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5]\n",
      "(16,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(out.cpu().detach().numpy())\n",
    "df_pca = pca.transform(out.cpu().detach().numpy())\n",
    "print(df_pca.shape)\n",
    "print(df_pca)\n",
    "print(targets.numpy())\n",
    "print(targets.numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf1e451c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.979976\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.773366\n",
      "\n",
      "Test set : Average loss : 0.0229, Accuracy : 5345/10000 (53%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.739937\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.684023\n",
      "\n",
      "Test set : Average loss : 0.0214, Accuracy : 5786/10000 (58%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.655862\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.743216\n",
      "\n",
      "Test set : Average loss : 0.0199, Accuracy : 6264/10000 (63%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.568274\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.606107\n",
      "\n",
      "Test set : Average loss : 0.0191, Accuracy : 6483/10000 (65%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.648004\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.550344\n",
      "\n",
      "Test set : Average loss : 0.0183, Accuracy : 6626/10000 (66%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.681226\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.691523\n",
      "\n",
      "Test set : Average loss : 0.0181, Accuracy : 6800/10000 (68%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.543842\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.584434\n",
      "\n",
      "Test set : Average loss : 0.0179, Accuracy : 6757/10000 (68%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.521394\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.560966\n",
      "\n",
      "Test set : Average loss : 0.0176, Accuracy : 6867/10000 (69%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.563265\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.552615\n",
      "\n",
      "Test set : Average loss : 0.0175, Accuracy : 6946/10000 (69%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.518798\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.498309\n",
      "\n",
      "Test set : Average loss : 0.0174, Accuracy : 6913/10000 (69%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(log_interval, model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52822cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
